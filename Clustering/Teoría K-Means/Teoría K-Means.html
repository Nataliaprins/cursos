

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="y" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="y" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Teoría K-Means &mdash; Material Curso 1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="Material Curso 1 documentation" href="../../index.html"/>
        <link rel="up" title="Clustering" href="../index.html"/>
        <link rel="next" title="K-Means en R" href="../K-Means%20en%20R/K-Means%20en%20R.html"/>
        <link rel="prev" title="Clustering" href="../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Material Curso
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Análisis de datos:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Introducci%C3%B3n%20a%20R/index.html">Introducción a R</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ggplot2/index.html">ggplot2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Regresi%C3%B3n%20lineal/index.html">Regresión lineal</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Análisis exploratorio de datos:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../An%C3%A1lisis%20gr%C3%A1fico/index.html">Análisis gráfico</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../An%C3%A1lisis%20multivariado%20de%20datos/index.html">Análisis multivariado de datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../An%C3%A1lisis%20de%20componentes%20principales/index.html">Análisis de componentes principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../An%C3%A1lisis%20factorial/index.html">Análisis Factorial</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Clustering</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Teoría K-Means</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#aprendizaje-no-supervisado">Aprendizaje no supervisado:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#k-means">K-Means:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distancias">Distancias:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distancias-manhattan">Distancias Manhattan:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distancia-euclidea">Distancia Euclídea:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distancia-minkowski">Distancia Minkowski:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#escalamiento-de-variables">Escalamiento de variables:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#numero-optimo-de-clusters">Número óptimo de clusters:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metodo-del-codo-elbow-method">Método del codo (Elbow method):</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metodo-de-la-silueta-average-silhouette-method">Método de la silueta (Average silhouette method):</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metodo-del-gap-estadistico-gap-statistic-method">Método del gap estadístico (Gap statistic method):</a></li>
<li class="toctree-l3"><a class="reference internal" href="#desventajas-del-k-means">Desventajas del K-Means:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../K-Means%20en%20R/K-Means%20en%20R.html">K-Means en R</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Clustering%20jerarquico/Cluster_jer%C3%A1rquico.html">Clusters Jerárquicos</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Cluster%20jer%C3%A1rquico%20en%20R/Cluster%20jer%C3%A1rquico%20en%20R.html">Cluster jerárquico en R</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Taller%20clustering/Taller%20An%C3%A1lisis%20Clustering.html">Taller Análisis Clustering</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Escalamiento%20Multidimensional/index.html">Escalamiento Multidimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Datos%20faltantes/index.html">Datos faltantes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../An%C3%A1lsis%20de%20correspondencia/index.html">Análisis de correspondencia</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Teoría de portafolios:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Generalidades%20del%20Mercado%20Colombiano/index.html">Generalidades del Mercado Colombiano</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Manejo%20de%20R-Studio/index.html">Conceptos Previos: Introducción a R y R-Studio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Conceptos%20B%C3%A1sicos%20de%20Estad%C3%ADstica/index.html">Conceptos Básicos de Estadística</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Teor%C3%ADa%20de%20Portafolios/index.html">Teoría de Portafolios</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Selecci%C3%B3n%20de%20Carteras/index.html">Selección de Cartera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mercado%20de%20Renta%20Fija/index.html">Mercado de Renta fija</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Modelación:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Caracterizaci%C3%B3n%20de%20las%20series%20de%20tiempo/index.html">Caracterización de las series de tiempo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Modelos%20autorregresivos/index.html">ARIMA: autoregresivo integrado de medias móviles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Deep%20Learning%20para%20pron%C3%B3stico/index.html">Deep Learning para pronóstico</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Material Curso</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Clustering</a> &raquo;</li>
        
      <li>Teoría K-Means</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Clustering/Teoría K-Means/Teoría K-Means.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="teoria-k-means">
<h1>Teoría K-Means<a class="headerlink" href="#teoria-k-means" title="Permalink to this headline">¶</a></h1>
<p>Se busca agrupar las observaciones semejantes y dividir en grupos que
tengan características diferentes. Con la agrupación o clustering los
puntos de datos de una muestra grande se agrupan según sus similitudes.
Así que la agrupación comprende e identifica patrones ocultos de los
datos. Los métodos para agrupar los datos hacen parte de los métodos de
aprendizaje no supervisado del Machine Learning.</p>
<div class="section" id="aprendizaje-no-supervisado">
<h2>Aprendizaje no supervisado:<a class="headerlink" href="#aprendizaje-no-supervisado" title="Permalink to this headline">¶</a></h2>
<p>Los modelos que realizan tareas de agrupamiento o Clustering hacen parte
de los modelos de aprendizaje automático no supervisado. Estos modelos
se aplican a los conjuntos de datos que tienen una etiqueta o clase, es
decir, que no tienen una variable <span class="math notranslate nohighlight">\(y\)</span> respuesta. Estos modelos se
entrenan sin ningún valor objetivo y solo aprenden de las
características inherentes que poseen los datos y los clasifica en uno o
más grupos. Con este agrupamiento no se está definiendo ninguna etiqueta
de clases, solo se están separando las muestras diferentes entre sí.</p>
</div>
<div class="section" id="k-means">
<h2>K-Means:<a class="headerlink" href="#k-means" title="Permalink to this headline">¶</a></h2>
<p>Este método agrupa los datos en <span class="math notranslate nohighlight">\(k\)</span> número de grupos o clusters en
un espacio de muestra dado. Las similitudes de los datos en un cluster
se miden en términos de <strong>distancia</strong> de los puntos desde el centro de
la agrupación. El centro del cluster se conoce como <strong>centroide</strong>.
Generalmente, antes de aplicar este método de asigna el valor de
<span class="math notranslate nohighlight">\(k\)</span> y el algoritmo K-Means intenta minimizar la suma de las
distancias entre los puntos de datos y el centroide del grupo al que
perteneces.</p>
<p>En el agrupamiento de K-Means, cada grupo está representado por su
centro (es decir, centroide) que corresponde a la media de los puntos
asignados al grupo.</p>
<p>Para medir las distancias de cada punto con su centroide podemos usar la
siguiente fórmula:</p>
<div class="math notranslate nohighlight">
\[D = \sqrt{\sum_{j=1}^k \sum_{i=1}^n{(C_j-P_i)^2}}\]</div>
<p>Donde,</p>
<p><span class="math notranslate nohighlight">\(P_i\)</span>: puntos dentro de un Cluster.</p>
<p><span class="math notranslate nohighlight">\(C_k\)</span>: centroide del Cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p><span class="math notranslate nohighlight">\(n\)</span>: cantidad de observaciones.</p>
<p><strong>Pasos para K-Means:</strong></p>
<ol class="arabic simple">
<li><p>Iniciar el modelo con un número de <span class="math notranslate nohighlight">\(k\)</span> cluters.</p></li>
<li><p>Aleatoriamente ubicar los <span class="math notranslate nohighlight">\(k\)</span> centroides entre los puntos de
datos.</p></li>
<li><p>Calcular los distancias entre todos los puntos de datos con cada
centroide y asignar los puntos al centroide más cercano.</p></li>
<li><p>Volver a calcular los centroides de los nuevos grupos.</p></li>
<li><p>Repetir los pasos 3 y 4 hasta que se cumplan los criterios para parar
las iteraciones.</p></li>
</ol>
<p>Para terminar y dejar de iterar el modelo se puede aplicar alguno de los
siguientes criterios:</p>
<ul class="simple">
<li><p>Si los nuevos centros de los clusters son iguales a los anteriores.</p></li>
<li><p>Si los puntos de datos de los clusters siguen siendo los mismos.</p></li>
<li><p>Cuando se alcanza el número máximo de iteraciones. Este criterio lo
define el analista.</p></li>
</ul>
<p><strong>Ejemplo:</strong></p>
<div class="figure align-default" id="id1">
<img alt="Puntos" src="../../_images/Puntos.PNG" />
<p class="caption"><span class="caption-text">Puntos</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>La anterior figura se representa por la siguiente matriz:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix}
Individuos &amp; Variable_1 &amp; Variable_2 \\
Individuo_1 &amp; 1 &amp; 1 \\
Individuo_2 &amp; 2 &amp; 1 \\
Individuo_3 &amp; 4 &amp; 3 \\
Individuo_4 &amp; 5 &amp; 4 \end{bmatrix}\end{split}\]</div>
<p>Tenemos <span class="math notranslate nohighlight">\(n = 4\)</span> individuos u observaciones y cada uno con
<span class="math notranslate nohighlight">\(p = 2\)</span> variables.</p>
<p><strong>Solución:</strong></p>
<p>Para <span class="math notranslate nohighlight">\(k=2\)</span> clusters, se inicializarán aleatoriamente. Supongamos
que el centroide 1 se ubica en <span class="math notranslate nohighlight">\(C1 = (1,1)\)</span> y el centroide 2 en
<span class="math notranslate nohighlight">\(C2 = (2,1)\)</span>. La siguiente figura muestra los puntos con los dos
centroides.</p>
<div class="figure align-default" id="id2">
<img alt="Centroides" src="../../_images/Centroides.PNG" />
<p class="caption"><span class="caption-text">Centroides</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Distancias de todos los puntos al centroide 1 -</strong> <span class="math notranslate nohighlight">\(C1 = (1,1)\)</span></p>
<p><span class="math notranslate nohighlight">\(D1:\sqrt{(1-1)^2+(1-1)^2}=0\)</span></p>
<p><span class="math notranslate nohighlight">\(D2:\sqrt{(1-2)^2+(1-1)^2}=1\)</span></p>
<p><span class="math notranslate nohighlight">\(D3:\sqrt{(1-4)^2+(1-3)^2}=3,61\)</span></p>
<p><span class="math notranslate nohighlight">\(D4:\sqrt{(1-5)^2+(1-4)^2}=5\)</span></p>
<p><strong>Distancias de todos los puntos al centroide 2 -</strong> <span class="math notranslate nohighlight">\(C2 = (2,1)\)</span></p>
<p><span class="math notranslate nohighlight">\(D1:\sqrt{(2-1)^2+(1-1)^2}=1\)</span></p>
<p><span class="math notranslate nohighlight">\(D2:\sqrt{(2-2)^2+(1-1)^2}=0\)</span></p>
<p><span class="math notranslate nohighlight">\(D3:\sqrt{(2-4)^2+(1-3)^2}=2,83\)</span></p>
<p><span class="math notranslate nohighlight">\(D4:\sqrt{(2-5)^2+(1-4)^2}=4,24\)</span></p>
<p>Resumiendo:</p>
<div class="math notranslate nohighlight">
\[\begin{split} D = \begin{bmatrix}
0 &amp; 1 &amp; 3,61 &amp; 5 \\
1 &amp; 0 &amp; 2,83 &amp; 4,24  \end{bmatrix} \begin{bmatrix}
C1(grupo1) \\
C2(grupo2)  \end{bmatrix}\end{split}\]</div>
<p>Con el criterio de asignar el centroide más cercano a cada punto, solo
el punto (1,1) pertenece a C1, los demás puntos pertenecen a C2.</p>
<div class="figure align-default" id="id3">
<img alt="Iteracion1" src="../../_images/Iteracion1.PNG" />
<p class="caption"><span class="caption-text">Iteracion1</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>Luego, se determinan los nuevos centroides. Como el cluster 1 solo tiene
un solo individuo, el centroide 1 sigue siendo el mismo. Para el cluster
2, el nuevo centroide se calcula como la media (means) de los puntos
así:</p>
<div class="math notranslate nohighlight">
\[ C2 = \begin{pmatrix}
\frac{2+4+5}{3} &amp;
\frac{1+3+4}{3}  \end{pmatrix} = (3,67; 2,67)\]</div>
<p>Nuevamente, la matriz de distancias sería:</p>
<div class="math notranslate nohighlight">
\[\begin{split} D = \begin{pmatrix}
0 &amp; 1 &amp; 3,61 &amp; 5 \\
3,14 &amp; 2,36 &amp; 0,47 &amp; 1,89  \end{pmatrix} \begin{pmatrix}
C1(grupo1) \\
C2(grupo2)  \end{pmatrix}\end{split}\]</div>
<p>Con el criterio de la mínima distancia al centroide, ahora los puntos
(1,1) y (2,1) pertenecen al cluster 1 y los puntos (4,3) t (5,4)
pertenecen al cluster 2.</p>
<div class="figure align-default" id="id4">
<img alt="Iteracion2" src="../../_images/Iteracion2.PNG" />
<p class="caption"><span class="caption-text">Iteracion2</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>Los dos nuevos centroides serán:</p>
<div class="math notranslate nohighlight">
\[ C1 = \begin{pmatrix}
\frac{1+2}{2} &amp;
\frac{1+1}{2}  \end{pmatrix} = (1.51; 1)\]</div>
<div class="math notranslate nohighlight">
\[ C2 = \begin{pmatrix}
\frac{4+5}{2} &amp;
\frac{3+4}{2}  \end{pmatrix} = (4,5; 3,5)\]</div>
<p>Para estos nuevos centroides la matriz de distancias es:</p>
<div class="math notranslate nohighlight">
\[\begin{split} D = \begin{pmatrix}
0,5 &amp; 0,5 &amp; 3,2 &amp; 4,61 \\
4,3 &amp; 3,54 &amp; 0,71 &amp; 0,71  \end{pmatrix} \begin{pmatrix}
C1(grupo1) \\
C2(grupo2)  \end{pmatrix}\end{split}\]</div>
<p>Nuevamente con el criterio de distancia mínima a los centroides, los
puntos (1,1) y (2,1) siguen perteneciendo al cluster 1 y los puntos
(4,3) y (5,4) al cluster 2. Como no hay cambios en los individuos en los
clusters, por tanto, el algoritmo K-Means converge en este punto.</p>
<div class="figure align-default" id="id5">
<img alt="Iteracion3" src="../../_images/Iteracion3.PNG" />
<p class="caption"><span class="caption-text">Iteracion3</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>La clasificación queda de la siguiente manera:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 26%" />
<col style="width: 26%" />
<col style="width: 26%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Individuos</strong></p></th>
<th class="head"><p><strong>Variable 1</strong></p></th>
<th class="head"><p><strong>Variable 2</strong></p></th>
<th class="head"><p><strong>Cluster</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Individuo 1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Individuo 2</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>Individuo 3</p></td>
<td><p>4</p></td>
<td><p>3</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>Individuo 4</p></td>
<td><p>5</p></td>
<td><p>4</p></td>
<td><p>2</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="distancias">
<h2>Distancias:<a class="headerlink" href="#distancias" title="Permalink to this headline">¶</a></h2>
<p>Veremos las distancias Manhattan, Euclídea y Minkowsky.</p>
<p>Tenemos <span class="math notranslate nohighlight">\(n\)</span> individuos u observaciones y cada uno con <span class="math notranslate nohighlight">\(p\)</span>
variables.</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1p} \\
x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2p} \\
.      &amp;   .    &amp;   .    &amp;  .  &amp;   .     \\
.      &amp;   .    &amp;   .    &amp;  .  &amp;   .     \\
x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{np} \end{bmatrix}\end{split}\]</div>
<p>Un individuo será el vector:
<span class="math notranslate nohighlight">\(x_i = \begin{bmatrix} x_{i1}, &amp; x_{i2}, &amp; x_{i3} &amp; ..., &amp; x_{ip} \end{bmatrix}\)</span></p>
</div>
<div class="section" id="distancias-manhattan">
<h2>Distancias Manhattan:<a class="headerlink" href="#distancias-manhattan" title="Permalink to this headline">¶</a></h2>
<p>La distancia Manhattan entre el individuo <span class="math notranslate nohighlight">\(x_i\)</span> y el individuo
<span class="math notranslate nohighlight">\(x_j\)</span> se calcula con el valor absoluto de la resta entre las filas
<span class="math notranslate nohighlight">\(i\)</span> y <span class="math notranslate nohighlight">\(j\)</span>. Cada variable del individual <span class="math notranslate nohighlight">\(i\)</span> se resta
con las de individuo <span class="math notranslate nohighlight">\(j\)</span>, la sumatoria de estas restas en valor
absoluto es el cálculo de la distancia Manhattan.</p>
<div class="math notranslate nohighlight">
\[D_1(x_i, x_j) = |x_{i1}-x_{j1}|+|x_{i2}-x_{j2}|+...+|x_{ip}-x_{jp}|=\sum_{k=1}^p{|x_{ix}-x_{jx}|}\]</div>
</div>
<div class="section" id="distancia-euclidea">
<h2>Distancia Euclídea:<a class="headerlink" href="#distancia-euclidea" title="Permalink to this headline">¶</a></h2>
<p>Se restan las variables entre los dos individuos y se eleva al cuadrado.
La raíz cuadrada de todas estas distancias por variable es la distancia
Euclidiana.</p>
<div class="math notranslate nohighlight">
\[D_1(x_i, x_j) = \sqrt{(x_{i1}-x_{j1})^2+(x_{i2}-x_{j2})^2+...+(x_{ip}-x_{jp})^2}=\sqrt{\sum_{k=1}^p{(x_{ix}-x_{jx})^2}}\]</div>
<div class="figure align-default" id="id6">
<img alt="Distancias" src="../../_images/Distancias.PNG" />
<p class="caption"><span class="caption-text">Distancias</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="distancia-minkowski">
<h2>Distancia Minkowski:<a class="headerlink" href="#distancia-minkowski" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[D_1(x_i, x_j) = [(x_{i1}-x_{j1})^p+(x_{i2}-x_{j2})^p+...+(x_{ip}-x_{jp})^p]^{\frac{1}{p}}=(\sum_{k=1}^p{(x_{ix}-x_{jx})^p})^{\frac{1}{p}}\]</div>
<p>Si <span class="math notranslate nohighlight">\(p = 1\)</span>: Distancia Manhattan.</p>
<p>Si <span class="math notranslate nohighlight">\(p = 2\)</span>: Distancia Euclídea.</p>
<div class="figure align-default" id="id7">
<img alt="Minkowski" src="../../_images/Minkowski.PNG" />
<p class="caption"><span class="caption-text">Minkowski</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="escalamiento-de-variables">
<h2>Escalamiento de variables:<a class="headerlink" href="#escalamiento-de-variables" title="Permalink to this headline">¶</a></h2>
<p>Se recomienda estandarizar o normalizar las variables antes de calcular
las distancias, especialmente cuando tenemos grandes diferencias en las
unidades de las variables.</p>
<div class="math notranslate nohighlight">
\[Estandarización = X_{stand} = \frac{x_i-mín(x)}{máx(x)-mín(x)}\]</div>
<div class="math notranslate nohighlight">
\[Normalización = X_{norm} = \frac{x_i-\overline{x}}{\sigma_x}\]</div>
</div>
<div class="section" id="numero-optimo-de-clusters">
<h2>Número óptimo de clusters:<a class="headerlink" href="#numero-optimo-de-clusters" title="Permalink to this headline">¶</a></h2>
<p>Se emplean técnicas subjetivas.</p>
</div>
<div class="section" id="metodo-del-codo-elbow-method">
<h2>Método del codo (Elbow method):<a class="headerlink" href="#metodo-del-codo-elbow-method" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Se corre el algoritmo para diferentes <span class="math notranslate nohighlight">\(k\)</span> clusters, por
ejemplo, variando entre 1 y 10.</p></li>
<li><p>Para cada cluster calcular el WCSS.</p></li>
<li><p>Trazar una curva de WCSS con el número de clusters <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>La ubicación de la curva (codo) se considera como un indicador
apropiado para el agrupamiento.</p></li>
</ul>
<p>Se implementa la técnica del codo a la suma de las distancias al
cuadrado de cada punto de un cluster con respecto a su centroide. El
resultado se llama WCSS (Within-Cluster Sum of Square).</p>
<p>El valor máximo de WCSS es cuando solo se hace con un cluster, cuando
<span class="math notranslate nohighlight">\(k = 1\)</span>.</p>
<div class="math notranslate nohighlight">
\[WCSS = \sum_{P_i \in Cluster_1}{D(P_i, C_1)^2}+\sum_{P_i \in Cluster_2}{D(P_i, C_2)^2}+...+\sum_{P_i \in Cluster_k}{D(P_i, C_k)^2}\]</div>
<div class="math notranslate nohighlight">
\[WCSS = \sqrt{\sum_{j=1}^k \sum_{i=1}^n{(C_j-P_i)^2}}\]</div>
<p>Donde,</p>
<p><span class="math notranslate nohighlight">\(P_i\)</span>: puntos dentro de un Cluster.</p>
<p><span class="math notranslate nohighlight">\(C_k\)</span>: centroide del Cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p><span class="math notranslate nohighlight">\(n\)</span>: cantidad de observaciones.</p>
</div>
<div class="section" id="metodo-de-la-silueta-average-silhouette-method">
<h2>Método de la silueta (Average silhouette method):<a class="headerlink" href="#metodo-de-la-silueta-average-silhouette-method" title="Permalink to this headline">¶</a></h2>
<p>La silueta es una medida de qué tan cerca se compara el elemento con
otros elementos dentro del mismo grupo y qué tan suelto es con los
elementos de los grupos vecinos. Un valor de silueta cercano a 1 implica
que un elemento está el grupo correcto, mientras que un valor de silueta
cercano a -1 implica que está en el grupo incorrecto. El método de
silueta promedio calcula la silueta promedio de todos los elementos en
el conjunto de datos en función de diferentes valores para <span class="math notranslate nohighlight">\(k\)</span>. Si
la mayoría de los elementos tienen un valor alto, entonces el promedio
será alto y la configuración de agrupamiento se considera adecuada. Sin
embargo, si muchos puntos tienen un valor de silueta bajo, el promedio
también será bajo y la configuración de agrupamiento no es óptima.
Similar al método del codo, para usar el método de la silueta promedio,
representamos la silueta promedio contra diferentes valores de
<span class="math notranslate nohighlight">\(k\)</span>. El valor <span class="math notranslate nohighlight">\(k\)</span> correspondiente a la silueta promedio más
alta representa el número óptimo de grupos.</p>
<ul class="simple">
<li><p>Se corre el algoritmo para diferentes <span class="math notranslate nohighlight">\(k\)</span> clusters, por
ejemplo, variando entre 1 y 10.</p></li>
<li><p>Para cada cluster calcular la silueta promedio de las observaciones.</p></li>
<li><p>Trazar una curva de silueta promedio con el número de clusters
<span class="math notranslate nohighlight">\(k\)</span>. El número óptimo de clusters <span class="math notranslate nohighlight">\(k\)</span> es el que maximiza
la silueta promedio sobre un rango de valores posibles para
<span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>La ubicación de la curva (codo) se considera como un indicador
apropiado para el agrupamiento.</p></li>
</ul>
</div>
<div class="section" id="metodo-del-gap-estadistico-gap-statistic-method">
<h2>Método del gap estadístico (Gap statistic method):<a class="headerlink" href="#metodo-del-gap-estadistico-gap-statistic-method" title="Permalink to this headline">¶</a></h2>
<p>Compara la diferencia entre los grupos creados a partir de los datos
observados y los grupos creados a partir de un conjunto de datos
generado aleatoriamente, conocido como conjunto de datos de referencia.
Para un <span class="math notranslate nohighlight">\(k\)</span> dado, el gag estadístico es la diferencia en el WCSS
total para los datos observados y el del conjunto de datos de
referencia. El número óptimo de clusters se denota por el valor
<span class="math notranslate nohighlight">\(k\)</span> que produce el gap estadístico más grande.</p>
</div>
<div class="section" id="desventajas-del-k-means">
<h2>Desventajas del K-Means:<a class="headerlink" href="#desventajas-del-k-means" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Supone conocimiento previo del conjunto de datos para elegir el
número apropiado de clusters <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>El resultado de la clasificación es sensible a la selección aleatoria
inicial de centroides. Se pueden obtener diferentes agrupaciones en
diferentes ejecuciones del algoritmo.</p></li>
<li><p>Es sensible a los valores atípicos.</p></li>
<li><p>Si se reorganizan los datos, es posible obtener soluciones diferentes
cada vez que cambia el orden de los datos.</p></li>
</ol>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../K-Means%20en%20R/K-Means%20en%20R.html" class="btn btn-neutral float-right" title="K-Means en R" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../index.html" class="btn btn-neutral" title="Clustering" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Natalia Acevedo Prins.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'1',
            LANGUAGE:'y',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>