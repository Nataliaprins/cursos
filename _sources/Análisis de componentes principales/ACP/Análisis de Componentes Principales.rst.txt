Análisis de Componentes Principales
-----------------------------------

Suponga que tenemos una base de datos con cinco variables, cada variable
es una dimensión, entonces tenemos cinco dimensiones. Con la herramienta
de Análisis de Componentes Principales (ACP) o en inglés Principal
Components Analysis (PCA) podemos reducir la dimensionalidad de la base
de datos, por ejemplo, a dos dimensiones. Con el ACP se logra que estas
dos nuevas variables puedan resumir la mayor proporción de la
variabilidad de la información original. Estas nuevas variables se
llaman Componentes Principales. La siguiente figura muestra cómo las
variables originales se pueden expresar en dos nuevos ejes llamados
Componente 1 y Componente 2; sin embargo, el Componente Principal 1 es
el más importante porque que recoge más información. Este análisis se
realiza con la matriz de varianzas-covarianzas del conjunto de datos.

.. figure:: Componentes.png
   :alt: Componentes

   Componentes

Los Componentes Principales son los Eigenvectores de la matriz de
varianzas-covarianzas de las variables originales y se convertirán en
los nuevos ejes. El ángulo entre los Eigenvectores es de 90°, así que
los Eigenvectores son ortogonales entre sí.

La matriz de varianzas-covarianzas es simétrica, del orden :math:`nxn`,
las varianzas, :math:`(\sigma^2_{ij})`, de cada variable están en la
diagonal de la matriz, los demás valores son las covarianzas,
:math:`(\sigma_{ij})`.

.. math::

    \sum = \begin{bmatrix} \sigma^2_{11} & \sigma_{12} & ... & \sigma_{1p} \\ \sigma_{21} & \sigma^2_{22} & ... & \sigma_{2p} \\ ... & ... & ... & ... 
   \\ \sigma_{n1} & \sigma^2_{n2} & ... & \sigma_{np} \end{bmatrix} 

Para :math:`p` variables, se extraen :math:`p` Eigenvectores y :math:`p`
Eigenvalores. Estos Eigenvectores son las Componentes Principales y el
Eigenvector asociado a cada Componente Principal es la proporción de la
varianza total que ese componente puede explicar.

En el ACP no se generan variables nuevas, sino que se transforman en
nuevas combinaciones lineales. Por lo anterior, la varianza total de las
variables originales sigue siendo la misma.

Técnicamente, el Análisis de Componentes Principales implica la rotación
del sistema de coordenadas original a un nuevo sistema de coordenadas
con propiedades estadísticas inherentemente deseables. Más precisamente,
buscamos definir una transformación ortogonal a una matriz de covarianza
diagonal. Computacionalmente, ACP se reduce a resolver los valores
propios y los vectores propios de una matriz definida positiva mediante
un proceso generalmente denominado análisis de valores propios o
descomposición espectral.

Ejemplo:
~~~~~~~~

Supongamos la siguiente matriz de varianzas-covarianzas de un conjunto
de dos variables:

.. math::  \sum = \begin{bmatrix} 0,00761 & 0,00331\\ 0,00331 & 0,00840 \end{bmatrix} 

La varianza de la variable 1 es igual a 0,00761 y la varianza de la
variable 2 es 0,00840. La sumatoria de la varianza del conjunto de datos
es de 0,01601 :math:`(0,00761+0,00840)`.

Eigenvalores:
~~~~~~~~~~~~~

**Eigenvalores** :math:`\lambda_i`: varianzas de cada Componente.

.. math::  \lambda_1 = 0,0113385 

.. math::  \lambda_2 = 0,00467151 

El Componente Principal 1 es el más importante porque es el de mayor
Eigenvalor. Los Eigenvalores son las varianzas de cada Componente
Principal y la suma de los Eigenvalores es la suma de las varianzas de
las variables originales, es decir, es la varianza total, la cual es
0,01601 :math:`(0,0113385+0,00467151)`.

El Componente Principal 1 explica el 70,82% de la variabilidad total.
Recuerde que la sumatoria de los :math:`\lambda` es igual a la sumatoria
de las varianzas de las variables. El analista puede decidir en trabajar
solo con esta Componente y así se reduciría la dimensionalidad de los
datos. Este es el objetivo del ACP, **encontrar la menor cantidad de
componentes posibles que puedan explicar la mayor parte de la variación
original.** En otras palabras, con el ACP se busca representar la
:math:`p` variables en un número menor de variables (Componentes)
conformadas como **combinaciones lineales** de las originales y perder
la menor cantidad de información.

Al aplicar el ACP, las variables originales correlacionadas se
transforman en variables no correlacionadas.

-  **Proporción de la varianza de la Componente Principal 1:**

.. math::  \frac{\lambda_1}{\sum{\lambda_i}} = \frac{0,0113385}{0,0113385+0,00467151} = 0,7082 

-  **Proporción de la varianza de la Componente Principal 2:**

.. math::  \frac{\lambda_2}{\sum{\lambda_i}} = \frac{0,00467151}{0,0113385+0,00467151} = 0,2918 

Como la desviación estándar es la raíz cuadrada de la varianza, cada
Componente Principal tiene la siguiente desviación estándar.

-  **Desviación estándar Componente Principal 1:**

.. math::  \sqrt{\lambda_1} = \sqrt{0,0113385}  = 0,1065 

-  **Desviación estándar Componente Principal 2:**

.. math::  \sqrt{\lambda_2} = \sqrt{0,00467151}  = 0,0683 

Eigenvectores:
~~~~~~~~~~~~~~

**Eigenvectores:** Cargas de cada Componente.

.. math::  Eigenvector_1 = \begin{bmatrix} 0,6638921 \\ 0,7478284 \end{bmatrix} 

.. math::  Eigenvector_2 = \begin{bmatrix} -0,7478284 \\ 0,6638921 \end{bmatrix} 

Las cargas de un Componente Principal son los elementos del vector
propio que forman la componente. Cada componente es una combinación
lineal de las variables del conjunto de datos.

El primer elemento de :math:`Eigenvector_1` es 0,6638921, esta es la
carga o *score* para la primera variable original. El segundo elemento
es la carga que se le asigna a la segunda variable de la base de datos.

A la matriz que se conforma con los Eigenvectores se llama matriz de
rotación.

Cada vector propio debe tener una longitud igual a 1,0. Esto se
comprueba si la suma de cada elemento (cargas) al cuadrado es igual a
1,0. Esta condición es una restricción del modelo porque con el valor de
1,0 las varianzas no se modifican.

Unidades de las variables:
~~~~~~~~~~~~~~~~~~~~~~~~~~

Cuando se tienen variables con magnitudes grandes y otras pequeñas, se
tiene un problema porque las variables de magnitud mayor van a
predominar en la reducción de dimensionalidad y además, estas variables
tienen mayor varianza. También, la covarianza entre las variables será
mayor por la magnitud, siendo esto en muchos casos un resultado errado
porque la covarianza estaría afectada por las unidades de las variables
con unidades mayores y no por el co-movimiento.

Para solucionar esto tenemos dos opciones:

**1. Cambio de escala de las variables:** puede cambiar las unidades de
cada variable con una de las siguientes dos opciones:

.. math::  Estandarización = X_{stand} = \frac{x_i-mín(x)}{máx(x)-mín(x)}  

.. math::  Normalización = X_{norm} = \frac{x_i-\overline{x}}{\sigma_x}  

**2. Matriz de correlaciones:** realizar el ACP sobre la matriz de
coeficientes de correlación en lugar de la matriz de
varianzas-covarianzas.

.. math::

    \sum = \begin{bmatrix} \rho_{11} & \rho_{12} & ... & \rho_{1p} \\ \rho_{21} & \rho_{22} & ... & \rho_{2p} \\ ... & ... & ... & ... 
   \\ \rho_{n1} & \rho_{n2} & ... & \rho_{np} \end{bmatrix} 

Como la diagonal tiene valores de 1,0, la suma de la diagonal es igual a
:math:`p`, cantidad de variables.

**Matriz de varianzas-covarianzas del ejemplo:**

.. code:: r

    A = matrix(c(0.00761, 0.00331, 0.00331, 0.00840), ncol = 2)
    print(A)


.. parsed-literal::

            [,1]    [,2]
    [1,] 0.00761 0.00331
    [2,] 0.00331 0.00840
    

**Eigenvalores e Eigenvectores:**

.. code:: r

    eigen = eigen(A)
    print(eigen)


.. parsed-literal::

    eigen() decomposition
    $values
    [1] 0.011338485 0.004671515
    
    $vectors
              [,1]       [,2]
    [1,] 0.6638921 -0.7478284
    [2,] 0.7478284  0.6638921
    
    

:math:`\lambda_1`: varianza de la Componente Principal 1.

.. code:: r

    lambda_1 = eigen$values[1]
    print(lambda_1)


.. parsed-literal::

    [1] 0.01133849
    

:math:`\lambda_2`: varianza de la Componente Principal 2.

.. code:: r

    lambda_2 = eigen$values[2]
    print(lambda_2)


.. parsed-literal::

    [1] 0.004671515
    

**PC1:** cargas de la Componente Principal 1.

.. code:: r

    pc1 = eigen$vectors[c(1,2)]
    print(pc1)


.. parsed-literal::

    [1] 0.6638921 0.7478284
    

**PC2:** cargas de la Componente Principal 2.

.. code:: r

    pc2 = eigen$vectors[c(3,4)]
    print(pc2)


.. parsed-literal::

    [1] -0.7478284  0.6638921
    

**Prueba:**

La suma de las cargas de cada componente al cuadrado debe ser igual a
1,0:

.. code:: r

    print(sum(pc1^2))


.. parsed-literal::

    [1] 1
    

.. code:: r

    print(sum(pc2^2))


.. parsed-literal::

    [1] 1
    

Si los vectores son ortogonales, entonces el producto escalar de los
vectores es igual a cero:

.. code:: r

    print(pc1 %*% pc2)


.. parsed-literal::

         [,1]
    [1,]    0
    

:math:`\sum{\lambda_i} = \sum{var_i}`: la suma de las varianzas de las
variables es igual a la suma de los Eigenvalores.

.. code:: r

    lambda_1 + lambda_2



.. raw:: html

    0.01601


.. code:: r

    A[1,1] + A[2,2]



.. raw:: html

    0.01601


Ejemplo ACP con EAM 2019:
~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: r

    datos <- read.csv("EAM_2019.csv", sep = ";", dec = ",", header = T)
    print(head(datos))


.. parsed-literal::

      ï..ciiu personal_mujer personal_hombre gasto_personal gasto_financiero
    1    1051             36             140        9352991          3240559
    2    1030             40             176        7334998          1468298
    3    3290             15             172        6668544          1547666
    4    3091             88             373       22088759         35203208
    5    3290             18              53        5219070          2861773
    6    3290             18              53        5219070          2861773
      costos_gastos_produccion gastos_adm_ventas inversion_AF    ventas
    1                  6846304          22920307      4979745 192609248
    2                  5941761          12310286      5615593 115741258
    3                  6996020           2564695       773444  44580029
    4                  4175751         171278876     10501572 162509864
    5                 11037978          13691919      6423171  87324374
    6                 11037978          13691919      6423171  87324374
    

.. code:: r

    datos <- datos[, 2:9]
    print(head(datos))


.. parsed-literal::

      personal_mujer personal_hombre gasto_personal gasto_financiero
    1             36             140        9352991          3240559
    2             40             176        7334998          1468298
    3             15             172        6668544          1547666
    4             88             373       22088759         35203208
    5             18              53        5219070          2861773
    6             18              53        5219070          2861773
      costos_gastos_produccion gastos_adm_ventas inversion_AF    ventas
    1                  6846304          22920307      4979745 192609248
    2                  5941761          12310286      5615593 115741258
    3                  6996020           2564695       773444  44580029
    4                  4175751         171278876     10501572 162509864
    5                 11037978          13691919      6423171  87324374
    6                 11037978          13691919      6423171  87324374
    

**Matriz de varianzas-covarianzas:**

.. code:: r

    A = cov(datos)
    print(A)


.. parsed-literal::

                             personal_mujer personal_hombre gasto_personal
    personal_mujer             3.739215e+04    1.997898e+04   1.862377e+09
    personal_hombre            1.997898e+04    5.062854e+04   3.599630e+09
    gasto_personal             1.862377e+09    3.599630e+09   3.545755e+14
    gasto_financiero           5.104016e+07    7.252934e+07   1.365787e+13
    costos_gastos_produccion   1.221006e+09    2.766463e+09   3.967152e+14
    gastos_adm_ventas          2.949389e+09    1.403570e+10   2.183437e+15
    inversion_AF               1.108302e+09    2.779767e+09   2.928899e+14
    ventas                     4.820786e+09    3.646053e+10   5.053759e+15
                             gasto_financiero costos_gastos_produccion
    personal_mujer               5.104016e+07             1.221006e+09
    personal_hombre              7.252934e+07             2.766463e+09
    gasto_personal               1.365787e+13             3.967152e+14
    gasto_financiero             1.967009e+13             2.705401e+13
    costos_gastos_produccion     2.705401e+13             1.209958e+15
    gastos_adm_ventas            1.149462e+14             6.138182e+15
    inversion_AF                 4.619610e+13             8.194616e+14
    ventas                       1.874646e+14             1.326626e+16
                             gastos_adm_ventas inversion_AF       ventas
    personal_mujer                2.949389e+09 1.108302e+09 4.820786e+09
    personal_hombre               1.403570e+10 2.779767e+09 3.646053e+10
    gasto_personal                2.183437e+15 2.928899e+14 5.053759e+15
    gasto_financiero              1.149462e+14 4.619610e+13 1.874646e+14
    costos_gastos_produccion      6.138182e+15 8.194616e+14 1.326626e+16
    gastos_adm_ventas             5.944858e+16 4.009155e+15 1.178659e+17
    inversion_AF                  4.009155e+15 1.547354e+16 8.777248e+15
    ventas                        1.178659e+17 8.777248e+15 2.478986e+17
    

.. code:: r

    eigen <- eigen(A)
    eigen



.. parsed-literal::

    eigen() decomposition
    $values
    [1] 3.056986e+17 1.516353e+16 2.811474e+15 5.205976e+14 1.924801e+14
    [6] 1.831384e+13 2.326745e+04 1.157060e+04
    
    $vectors
                  [,1]          [,2]          [,3]          [,4]          [,5]
    [1,] -1.878137e-08  6.239123e-08 -1.183730e-07 -2.970771e-06 -6.900966e-06
    [2,] -1.280982e-07  1.035893e-07  1.222791e-06 -2.739607e-06 -1.153716e-05
    [3,] -1.807568e-02  7.757045e-03  9.011061e-02 -3.506543e-01 -9.312296e-01
    [4,] -7.244017e-04  2.571890e-03 -7.326667e-03 -4.089698e-02 -2.462619e-02
    [5,] -4.802479e-02  2.289081e-02  9.528623e-02 -9.260466e-01  3.601660e-01
    [6,] -4.324727e-01 -2.931718e-02 -8.956921e-01 -8.763492e-02 -4.506920e-02
    [7,] -3.333367e-02  9.989790e-01 -1.882518e-02  2.385429e-02 -1.764564e-03
    [8,] -8.995680e-01 -2.430295e-02  4.244151e-01  9.776436e-02  2.123638e-02
                  [,6]          [,7]          [,8]
    [1,]  2.949406e-06  9.999175e-01 -1.284377e-02
    [2,]  2.607919e-06 -1.284377e-02 -9.999175e-01
    [3,]  3.668912e-02 -7.412828e-06  1.200959e-05
    [4,] -9.988294e-01  2.624774e-06 -2.251206e-06
    [5,]  2.843181e-02 -3.586853e-07 -1.414615e-06
    [6,]  1.150768e-02 -7.208121e-07 -2.435771e-07
    [7,]  1.801331e-03 -1.123465e-08  4.458666e-08
    [8,] -7.049896e-03  4.938902e-07  9.412163e-08
    


**Eigenvalores:**

Son las varianzas de las Componentes Principales.

.. code:: r

    lambda <- eigen$values
    print(lambda)


.. parsed-literal::

    [1] 3.056986e+17 1.516353e+16 2.811474e+15 5.205976e+14 1.924801e+14
    [6] 1.831384e+13 2.326745e+04 1.157060e+04
    

Las desviaciones estándar de las Componentes Principales son:

.. code:: r

    sqrt(lambda)



.. raw:: html

    <style>
    .list-inline {list-style: none; margin:0; padding: 0}
    .list-inline>li {display: inline-block}
    .list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
    </style>
    <ol class=list-inline><li>552900143.548993</li><li>123140268.027212</li><li>53023338.9350048</li><li>22816608.8714465</li><li>13873720.756864</li><li>4279467.7380434</li><li>152.53671096197</li><li>107.566722167472</li></ol>
    


.. code:: r

    plot(lambda, type = "l")



.. image:: output_46_0.png
   :width: 420px
   :height: 420px


**Problemas con las unidades de las variables:**

La primera Componente Principal explica la mayor cantidad de la
variabilidad del conjunto de datos, pero tenemos un problema con las
unidades de las variables, tenemos dos variables ``personal_mujer`` y
``personal_hombre`` con magnitudes mucho menores que las demás. Esto
significa que la primer Componente Principal está otorgando mayor
importancia a las variables de mayor magnitud. Para solucionar este
problema podemos cambiar la escala de las variables originales o aplicar
el ACP a la matriz de correlaciones. En esta sesión se aplicar a la
matriz de correlaciones.

Con este problema la técnica del codo indica que sólo se necesita una
Componente Principal.

**Eigenvectores:**

Son las cargas de cada Componente Principal.

.. code:: r

    cargas <- eigen$vectors
    print(cargas)


.. parsed-literal::

                  [,1]          [,2]          [,3]          [,4]          [,5]
    [1,] -1.878137e-08  6.239123e-08 -1.183730e-07 -2.970771e-06 -6.900966e-06
    [2,] -1.280982e-07  1.035893e-07  1.222791e-06 -2.739607e-06 -1.153716e-05
    [3,] -1.807568e-02  7.757045e-03  9.011061e-02 -3.506543e-01 -9.312296e-01
    [4,] -7.244017e-04  2.571890e-03 -7.326667e-03 -4.089698e-02 -2.462619e-02
    [5,] -4.802479e-02  2.289081e-02  9.528623e-02 -9.260466e-01  3.601660e-01
    [6,] -4.324727e-01 -2.931718e-02 -8.956921e-01 -8.763492e-02 -4.506920e-02
    [7,] -3.333367e-02  9.989790e-01 -1.882518e-02  2.385429e-02 -1.764564e-03
    [8,] -8.995680e-01 -2.430295e-02  4.244151e-01  9.776436e-02  2.123638e-02
                  [,6]          [,7]          [,8]
    [1,]  2.949406e-06  9.999175e-01 -1.284377e-02
    [2,]  2.607919e-06 -1.284377e-02 -9.999175e-01
    [3,]  3.668912e-02 -7.412828e-06  1.200959e-05
    [4,] -9.988294e-01  2.624774e-06 -2.251206e-06
    [5,]  2.843181e-02 -3.586853e-07 -1.414615e-06
    [6,]  1.150768e-02 -7.208121e-07 -2.435771e-07
    [7,]  1.801331e-03 -1.123465e-08  4.458666e-08
    [8,] -7.049896e-03  4.938902e-07  9.412163e-08
    

**Proporción de la varianza de cada Componente Principal:**

La varianza que cada Componente Principal logra explicar por separado:

.. math::  \frac{\lambda_i}{\sum{\lambda_i}} 

.. code:: r

    prop_var <- lambda/sum(lambda)
    print(prop_var)


.. parsed-literal::

    [1] 9.423363e-01 4.674258e-02 8.666558e-03 1.604777e-03 5.933329e-04
    [6] 5.645365e-05 7.172347e-14 3.566715e-14
    

Proporción de varianza acumulada al aumentar Componentes Principales:

.. code:: r

    prop_var_acum <- cumsum(lambda)/sum(lambda)
    print(prop_var_acum)


.. parsed-literal::

    [1] 0.9423363 0.9890789 0.9977454 0.9993502 0.9999435 1.0000000 1.0000000
    [8] 1.0000000
    

La Componente Principal 1 explica el 94,23% de la varianza del conjunto
de datos.

**ACP con la matriz de correlaciones:**

Se realiza el mismo procedimiento, pero con la matriz de coeficientes de
correlación:

.. code:: r

    A = cor(datos)
    print(A)


.. parsed-literal::

                             personal_mujer personal_hombre gasto_personal
    personal_mujer               1.00000000      0.45918261      0.5114733
    personal_hombre              0.45918261      1.00000000      0.8495829
    gasto_personal               0.51147327      0.84958291      1.0000000
    gasto_financiero             0.05951392      0.07267965      0.1635405
    costos_gastos_produccion     0.18152747      0.35346147      0.6056747
    gastos_adm_ventas            0.06255630      0.25583836      0.4755711
    inversion_AF                 0.04607581      0.09931522      0.1250418
    ventas                       0.05007149      0.32545296      0.5390426
                             gasto_financiero costos_gastos_produccion
    personal_mujer                 0.05951392                0.1815275
    personal_hombre                0.07267965                0.3534615
    gasto_personal                 0.16354049                0.6056747
    gasto_financiero               1.00000000                0.1753652
    costos_gastos_produccion       0.17536517                1.0000000
    gastos_adm_ventas              0.10629691                0.7237423
    inversion_AF                   0.08373504                0.1893863
    ventas                         0.08489441                0.7659955
                             gastos_adm_ventas inversion_AF     ventas
    personal_mujer                   0.0625563   0.04607581 0.05007149
    personal_hombre                  0.2558384   0.09931522 0.32545296
    gasto_personal                   0.4755711   0.12504179 0.53904261
    gasto_financiero                 0.1062969   0.08373504 0.08489441
    costos_gastos_produccion         0.7237423   0.18938634 0.76599555
    gastos_adm_ventas                1.0000000   0.13218655 0.97091339
    inversion_AF                     0.1321865   1.00000000 0.14171846
    ventas                           0.9709134   0.14171846 1.00000000
    

.. code:: r

    eigen <- eigen(A)
    eigen



.. parsed-literal::

    eigen() decomposition
    $values
    [1] 3.56505893 1.52081291 1.02309043 0.91630483 0.55575693 0.30401093 0.09273024
    [8] 0.02223479
    
    $vectors
               [,1]        [,2]        [,3]        [,4]         [,5]        [,6]
    [1,] -0.2106928 -0.57578375  0.02214712  0.02361349  0.772202925  0.11726775
    [2,] -0.3585481 -0.46745657  0.08352398  0.04828165 -0.542176918  0.14380998
    [3,] -0.4548743 -0.32937778  0.04737869 -0.02403973 -0.253374106 -0.09286009
    [4,] -0.1115576 -0.01348448 -0.74003083 -0.65388863 -0.021041260  0.09724572
    [5,] -0.4461470  0.21007377 -0.01448462 -0.02746956  0.136769241 -0.82513357
    [6,] -0.4336228  0.39544860  0.11741035 -0.03425391  0.155556129  0.41861887
    [7,] -0.1238028  0.05636775 -0.64060409  0.75295144 -0.003127729  0.06144540
    [8,] -0.4530927  0.37094199  0.13518240 -0.00996259  0.046822464  0.29601441
                [,7]          [,8]
    [1,] -0.09769967  0.0586448480
    [2,] -0.57271625 -0.0312928834
    [3,]  0.77820633 -0.0578830139
    [4,] -0.03932248  0.0266717602
    [5,] -0.23007970 -0.0580479890
    [6,] -0.04299362 -0.6628311912
    [7,]  0.01999398 -0.0009877992
    [8,] -0.01070724  0.7408130522
    


**Eigenvalores:**

Son las varianzas de las Componentes Principales.

.. code:: r

    lambda <- eigen$values
    print(lambda)


.. parsed-literal::

    [1] 3.56505893 1.52081291 1.02309043 0.91630483 0.55575693 0.30401093 0.09273024
    [8] 0.02223479
    

Las desviaciones estándar de las Componentes Principales son:

.. code:: r

    sqrt(lambda)



.. raw:: html

    <style>
    .list-inline {list-style: none; margin:0; padding: 0}
    .list-inline>li {display: inline-block}
    .list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
    </style>
    <ol class=list-inline><li>1.88813636414594</li><li>1.23321243323159</li><li>1.01147932912133</li><li>0.957238127235179</li><li>0.745491067526704</li><li>0.551371866199051</li><li>0.304516400845997</li><li>0.149113361098913</li></ol>
    


.. code:: r

    screeplot(princomp(datos, cor = T),
              main = "Screeplot", col = "blue", type = "lines",
              pch = 19)



.. image:: output_68_0.png
   :width: 420px
   :height: 420px


En otra sesión podrá comprobar que estos resultados con la matriz de
correlación son igual que si se escalaran los datos.

En este resultado la técnica del codo ya no muestra que se debe
seleccionar una sola Componente Principal. Se solucionó el problema de
las unidades de las variables.

**Eigenvectores:**

Son las cargas de cada Componente Principal.

.. code:: r

    cargas <- eigen$vectors
    print(cargas)


.. parsed-literal::

               [,1]        [,2]        [,3]        [,4]         [,5]        [,6]
    [1,] -0.2106928 -0.57578375  0.02214712  0.02361349  0.772202925  0.11726775
    [2,] -0.3585481 -0.46745657  0.08352398  0.04828165 -0.542176918  0.14380998
    [3,] -0.4548743 -0.32937778  0.04737869 -0.02403973 -0.253374106 -0.09286009
    [4,] -0.1115576 -0.01348448 -0.74003083 -0.65388863 -0.021041260  0.09724572
    [5,] -0.4461470  0.21007377 -0.01448462 -0.02746956  0.136769241 -0.82513357
    [6,] -0.4336228  0.39544860  0.11741035 -0.03425391  0.155556129  0.41861887
    [7,] -0.1238028  0.05636775 -0.64060409  0.75295144 -0.003127729  0.06144540
    [8,] -0.4530927  0.37094199  0.13518240 -0.00996259  0.046822464  0.29601441
                [,7]          [,8]
    [1,] -0.09769967  0.0586448480
    [2,] -0.57271625 -0.0312928834
    [3,]  0.77820633 -0.0578830139
    [4,] -0.03932248  0.0266717602
    [5,] -0.23007970 -0.0580479890
    [6,] -0.04299362 -0.6628311912
    [7,]  0.01999398 -0.0009877992
    [8,] -0.01070724  0.7408130522
    

**Proporción de la varianza de cada Componente Principal:**

La varianza que cada Componente Principal logra explicar por separado:

.. math::  \frac{\lambda_i}{\sum{\lambda_i}} 

.. code:: r

    prop_var <- lambda/sum(lambda)
    print(prop_var)


.. parsed-literal::

    [1] 0.445632366 0.190101613 0.127886304 0.114538104 0.069469616 0.038001367
    [7] 0.011591280 0.002779349
    

Proporción de varianza acumulada al aumentar Componentes Principales:

.. code:: r

    prop_var_acum <- cumsum(lambda)/sum(lambda)
    print(prop_var_acum)


.. parsed-literal::

    [1] 0.4456324 0.6357340 0.7636203 0.8781584 0.9476280 0.9856294 0.9972207
    [8] 1.0000000
    

La Componente Principal 2 explica el 44,56% de la varianza del conjunto
de datos. Si se usan las dos primeras Componentes Principales, se puede
explicar el 63,57% de la variabilidad total.

.. code:: r

    n <- dim(datos)[1]
    mu <- colMeans(datos)
    S <- cov(datos)

.. code:: r

    ones <- matrix(rep(1,n),nrow=n,ncol=1)
    datos.cen <- as.matrix(datos) - ones %*% mu
    datos.cen
    Dx <- diag(diag(S))
    head(Dx)
    Y <- datos.cen %*% solve(Dx)^(1/2)
    head(Y)



.. raw:: html

    <table class="dataframe">
    <caption>A matrix: 420 × 8 of type dbl</caption>
    <thead>
    	<tr><th scope=col>personal_mujer</th><th scope=col>personal_hombre</th><th scope=col>gasto_personal</th><th scope=col>gasto_financiero</th><th scope=col>costos_gastos_produccion</th><th scope=col>gastos_adm_ventas</th><th scope=col>inversion_AF</th><th scope=col>ventas</th></tr>
    </thead>
    <tbody>
    	<tr><td> -97.933333</td><td>-120.55952</td><td> -8848242</td><td>  542927.8</td><td> -8873424.4</td><td>-47470381</td><td>-15231998.3</td><td>  -9322297</td></tr>
    	<tr><td> -93.933333</td><td> -84.55952</td><td>-10866235</td><td>-1229333.2</td><td> -9777967.4</td><td>-58080402</td><td>-14596150.3</td><td> -86190287</td></tr>
    	<tr><td>-118.933333</td><td> -88.55952</td><td>-11532689</td><td>-1149965.2</td><td> -8723708.4</td><td>-67825993</td><td>-19438299.3</td><td>-157351516</td></tr>
    	<tr><td> -45.933333</td><td> 112.44048</td><td>  3887526</td><td>32505576.8</td><td>-11543977.4</td><td>100888188</td><td> -9710171.3</td><td> -39421681</td></tr>
    	<tr><td>-115.933333</td><td>-207.55952</td><td>-12982163</td><td>  164141.8</td><td> -4681750.4</td><td>-56698769</td><td>-13788572.3</td><td>-114607171</td></tr>
    	<tr><td>-115.933333</td><td>-207.55952</td><td>-12982163</td><td>  164141.8</td><td> -4681750.4</td><td>-56698769</td><td>-13788572.3</td><td>-114607171</td></tr>
    	<tr><td> -90.933333</td><td> -45.55952</td><td> -8814568</td><td>-2049686.2</td><td>-10273889.4</td><td>-65363031</td><td>-19847471.3</td><td>-168561472</td></tr>
    	<tr><td>-110.933333</td><td>-163.55952</td><td>-13444970</td><td>-2067063.2</td><td>-14096981.4</td><td>-57320166</td><td>-19946718.3</td><td>-142417492</td></tr>
    	<tr><td> -25.933333</td><td>-111.55952</td><td> -7120020</td><td>17692642.8</td><td>-13876201.4</td><td>  2366699</td><td> 30641843.7</td><td>-175204156</td></tr>
    	<tr><td>  63.066667</td><td>-190.55952</td><td> -9625892</td><td>-2155495.2</td><td>-11716938.4</td><td>-46229220</td><td>-19898921.3</td><td>-144589296</td></tr>
    	<tr><td> -81.933333</td><td>-102.55952</td><td>-14260980</td><td>-2084792.2</td><td>-14368440.4</td><td>-69603180</td><td>-14681641.3</td><td>-178763412</td></tr>
    	<tr><td>-116.933333</td><td>-215.55952</td><td>-16742030</td><td>-1915841.2</td><td>-15542454.4</td><td> -6531891</td><td>-19562703.3</td><td>-183366258</td></tr>
    	<tr><td>1161.066667</td><td> 215.44048</td><td> 21415142</td><td> 5065031.8</td><td> 21992863.6</td><td> 86146991</td><td>  -195466.3</td><td>  -9315245</td></tr>
    	<tr><td> -60.933333</td><td>-146.55952</td><td>-13152689</td><td>-1978187.2</td><td>-14592572.4</td><td>-45027116</td><td>-19603064.3</td><td>-179581278</td></tr>
    	<tr><td> -80.933333</td><td>-237.55952</td><td>-14794075</td><td> -622557.2</td><td>-15552928.4</td><td>-64743458</td><td>-19564763.3</td><td>-194943698</td></tr>
    	<tr><td>-110.933333</td><td>-156.55952</td><td> -9868525</td><td>-2179664.2</td><td> 18939041.6</td><td>-66312222</td><td>-15047568.3</td><td>-156986120</td></tr>
    	<tr><td>  51.066667</td><td>  25.44048</td><td> -2844102</td><td>-1782968.2</td><td> -9814222.4</td><td>-67894894</td><td>-16759666.3</td><td> -95440621</td></tr>
    	<tr><td>   7.066667</td><td> 511.44048</td><td> 12759844</td><td>-1210472.2</td><td>   443689.6</td><td> 39799286</td><td> -7539531.3</td><td>  34075337</td></tr>
    	<tr><td> -76.933333</td><td> -57.55952</td><td> -9141983</td><td>  886226.8</td><td>-14470724.4</td><td>-30447618</td><td>-19356119.3</td><td>-127418348</td></tr>
    	<tr><td>  12.066667</td><td> 158.44048</td><td> 20332689</td><td> 1353476.8</td><td> -1422324.4</td><td> -3301445</td><td>  2807914.7</td><td> 286147955</td></tr>
    	<tr><td> -84.933333</td><td>-168.55952</td><td>-16265068</td><td>-2069638.2</td><td>-11517343.4</td><td>-63226869</td><td>-20211743.3</td><td>-178577879</td></tr>
    	<tr><td> -46.933333</td><td>-141.55952</td><td>-13589021</td><td>-1950069.2</td><td>-12632507.4</td><td>-57588902</td><td>-20202076.3</td><td>-164060960</td></tr>
    	<tr><td>   1.066667</td><td>  15.44048</td><td>-13400772</td><td>-1829785.2</td><td> -9649809.4</td><td>-60713269</td><td>-20211743.3</td><td>-173608762</td></tr>
    	<tr><td> -62.933333</td><td> -97.55952</td><td> -9692368</td><td>-1773999.2</td><td>-14878042.4</td><td>-50459612</td><td>-19270706.3</td><td>-195283056</td></tr>
    	<tr><td> -77.933333</td><td> -90.55952</td><td> -8516768</td><td> 6776262.8</td><td>  3476371.6</td><td>-39909946</td><td>-16764846.3</td><td> 114463785</td></tr>
    	<tr><td>-108.933333</td><td>-113.55952</td><td>-10875171</td><td>-1828593.2</td><td>-13506918.4</td><td>-54520313</td><td>-20031030.3</td><td>-129990250</td></tr>
    	<tr><td>-110.933333</td><td>-150.55952</td><td>-12177575</td><td>-1043941.2</td><td>-14889677.4</td><td>-50791363</td><td>-19273291.3</td><td>-191691384</td></tr>
    	<tr><td>-109.933333</td><td> -44.55952</td><td> -7748678</td><td>-1615980.2</td><td>-12747352.4</td><td>-61044241</td><td>-18588007.3</td><td> -18847671</td></tr>
    	<tr><td> -87.933333</td><td>-128.55952</td><td>-13241759</td><td>-2154414.2</td><td>-11183424.4</td><td>-58557372</td><td>-18885919.3</td><td>-111318780</td></tr>
    	<tr><td> -21.933333</td><td>-139.55952</td><td>-10693745</td><td>-1884468.2</td><td>-13144838.4</td><td>-60072886</td><td>-19430209.3</td><td>-149442097</td></tr>
    	<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>
    	<tr><td> 268.066667</td><td>-159.55952</td><td> -7198875</td><td>-2025004.2</td><td>-13735673.4</td><td>-62277945.7</td><td> -19336230</td><td>-170872885</td></tr>
    	<tr><td> -16.933333</td><td>-152.55952</td><td>-12164148</td><td>-2001294.2</td><td>-13040562.4</td><td>-67402655.7</td><td> -20143047</td><td>-183896799</td></tr>
    	<tr><td> 268.066667</td><td>-159.55952</td><td> -7198875</td><td>-2025004.2</td><td>-13735673.4</td><td>-62277945.7</td><td> -19336230</td><td>-170872885</td></tr>
    	<tr><td> 571.066667</td><td> 847.44048</td><td> 74882102</td><td> -714230.2</td><td>   617369.6</td><td>185608010.3</td><td>  -2427312</td><td> 194835169</td></tr>
    	<tr><td>   3.066667</td><td> 110.44048</td><td> 13856972</td><td>-1866310.2</td><td>  3498496.6</td><td>  -620021.7</td><td> -14138976</td><td> 159230907</td></tr>
    	<tr><td> 571.066667</td><td> 847.44048</td><td> 74882102</td><td> -714230.2</td><td>   617369.6</td><td>185608010.3</td><td>  -2427312</td><td> 194835169</td></tr>
    	<tr><td>  12.066667</td><td> 158.44048</td><td> 20332689</td><td> 1353476.8</td><td> -1422324.4</td><td> -3301444.7</td><td>   2807915</td><td> 286147955</td></tr>
    	<tr><td> 571.066667</td><td> 847.44048</td><td> 74882102</td><td> -714230.2</td><td>   617369.6</td><td>185608010.3</td><td>  -2427312</td><td> 194835169</td></tr>
    	<tr><td> -80.933333</td><td>-184.55952</td><td>-10644577</td><td> -840527.2</td><td>-15268838.4</td><td>-62801987.7</td><td> -18719861</td><td>-157385039</td></tr>
    	<tr><td>  11.066667</td><td>  77.44048</td><td> 13820084</td><td>-1913283.2</td><td> -9614068.4</td><td>-20322962.7</td><td> -12564831</td><td>  20434383</td></tr>
    	<tr><td>-102.933333</td><td>-162.55952</td><td>-13456703</td><td> -770210.2</td><td>-11588523.4</td><td>-56945757.7</td><td>  -8153748</td><td> -91763885</td></tr>
    	<tr><td>  11.066667</td><td>  77.44048</td><td> 13820084</td><td>-1913283.2</td><td> -9614068.4</td><td>-20322962.7</td><td> -12564831</td><td>  20434383</td></tr>
    	<tr><td>  11.066667</td><td>  77.44048</td><td> 13820084</td><td>-1913283.2</td><td> -9614068.4</td><td>-20322962.7</td><td> -12564831</td><td>  20434383</td></tr>
    	<tr><td>-109.933333</td><td> -44.55952</td><td> -7748678</td><td>-1615980.2</td><td>-12747352.4</td><td>-61044240.7</td><td> -18588007</td><td> -18847671</td></tr>
    	<tr><td>  13.066667</td><td> 333.44048</td><td> 11448329</td><td> 5607969.8</td><td>  -132022.4</td><td> -6603198.7</td><td> -14709415</td><td> 364658609</td></tr>
    	<tr><td>  13.066667</td><td> 333.44048</td><td> 11448329</td><td> 5607969.8</td><td>  -132022.4</td><td> -6603198.7</td><td> -14709415</td><td> 364658609</td></tr>
    	<tr><td>  13.066667</td><td> 333.44048</td><td> 11448329</td><td> 5607969.8</td><td>  -132022.4</td><td> -6603198.7</td><td> -14709415</td><td> 364658609</td></tr>
    	<tr><td>  -3.933333</td><td> 227.44048</td><td>  1017244</td><td>-2076541.2</td><td> -4092283.4</td><td>-60230884.7</td><td>  -4736368</td><td> -37928379</td></tr>
    	<tr><td>  80.066667</td><td> 116.44048</td><td>  4903717</td><td> 2787591.8</td><td> 24698207.6</td><td> -5611750.7</td><td>1442759179</td><td>  -2176733</td></tr>
    	<tr><td> -93.933333</td><td>-129.55952</td><td>-12547871</td><td>-2137565.2</td><td>-13407820.4</td><td>-66217952.7</td><td> -19332931</td><td>-150095075</td></tr>
    	<tr><td> -55.933333</td><td>-101.55952</td><td> -7931405</td><td>-2006407.2</td><td>-14336425.4</td><td>-49376649.7</td><td>  -8592027</td><td>-146204442</td></tr>
    	<tr><td>-120.933333</td><td>-168.55952</td><td>-15810979</td><td>-2021662.2</td><td>-15289081.4</td><td>-67901462.7</td><td> -19394784</td><td>-189182927</td></tr>
    	<tr><td>-120.933333</td><td>-168.55952</td><td>-15810979</td><td>-2021662.2</td><td>-15289081.4</td><td>-67901462.7</td><td> -19394784</td><td>-189182927</td></tr>
    	<tr><td>-111.933333</td><td>-131.55952</td><td>  3196088</td><td>24778103.8</td><td> 12228515.6</td><td>154662468.3</td><td>   1795539</td><td>  31501235</td></tr>
    	<tr><td>-105.933333</td><td>-147.55952</td><td> -4199323</td><td> 5988384.8</td><td>  5340003.6</td><td>-30960090.7</td><td>   4938570</td><td> 177434155</td></tr>
    	<tr><td> -97.933333</td><td>-120.55952</td><td> -8848242</td><td>  542927.8</td><td> -8873424.4</td><td>-47470380.7</td><td> -15231998</td><td>  -9322297</td></tr>
    	<tr><td> 279.066667</td><td> 252.44048</td><td> 35732503</td><td>-1804305.2</td><td> -3548759.4</td><td> -6290792.7</td><td> -12337768</td><td>  85603150</td></tr>
    	<tr><td> 279.066667</td><td> 252.44048</td><td> 35732503</td><td>-1804305.2</td><td> -3548759.4</td><td> -6290792.7</td><td> -12337768</td><td>  85603150</td></tr>
    	<tr><td> -14.933333</td><td>  49.44048</td><td> 17418113</td><td>-2186324.2</td><td>  6092351.6</td><td> 43264215.3</td><td> -13912858</td><td> 153602706</td></tr>
    	<tr><td>-109.933333</td><td> -44.55952</td><td> -7748678</td><td>-1615980.2</td><td>-12747352.4</td><td>-61044240.7</td><td> -18588007</td><td> -18847671</td></tr>
    </tbody>
    </table>
    



.. raw:: html

    <table class="dataframe">
    <caption>A matrix: 6 × 8 of type dbl</caption>
    <tbody>
    	<tr><td>37392.15</td><td>    0.00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0</td><td>0</td></tr>
    	<tr><td>    0.00</td><td>50628.54</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0</td><td>0</td></tr>
    	<tr><td>    0.00</td><td>    0.00</td><td>3.545755e+14</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0</td><td>0</td></tr>
    	<tr><td>    0.00</td><td>    0.00</td><td>0.000000e+00</td><td>1.967009e+13</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0</td><td>0</td></tr>
    	<tr><td>    0.00</td><td>    0.00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>1.209958e+15</td><td>0.000000e+00</td><td>0</td><td>0</td></tr>
    	<tr><td>    0.00</td><td>    0.00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>5.944858e+16</td><td>0</td><td>0</td></tr>
    </tbody>
    </table>
    



.. raw:: html

    <table class="dataframe">
    <caption>A matrix: 6 × 8 of type dbl</caption>
    <tbody>
    	<tr><td>-0.5064543</td><td>-0.5358014</td><td>-0.4698970</td><td> 0.12241620</td><td>-0.2550975</td><td>-0.1946937</td><td>-0.12245090</td><td>-0.01872345</td></tr>
    	<tr><td>-0.4857687</td><td>-0.3758070</td><td>-0.5770650</td><td>-0.27718287</td><td>-0.2811017</td><td>-0.2382094</td><td>-0.11733929</td><td>-0.17310964</td></tr>
    	<tr><td>-0.6150542</td><td>-0.3935841</td><td>-0.6124579</td><td>-0.25928744</td><td>-0.2507934</td><td>-0.2781797</td><td>-0.15626560</td><td>-0.31603404</td></tr>
    	<tr><td>-0.2375405</td><td> 0.4997180</td><td> 0.2064519</td><td> 7.32916782</td><td>-0.3318718</td><td> 0.4137801</td><td>-0.07806062</td><td>-0.07917682</td></tr>
    	<tr><td>-0.5995399</td><td>-0.9224545</td><td>-0.6894340</td><td> 0.03700974</td><td>-0.1345932</td><td>-0.2325428</td><td>-0.11084712</td><td>-0.23018378</td></tr>
    	<tr><td>-0.5995399</td><td>-0.9224545</td><td>-0.6894340</td><td> 0.03700974</td><td>-0.1345932</td><td>-0.2325428</td><td>-0.11084712</td><td>-0.23018378</td></tr>
    </tbody>
    </table>
    


.. code:: r

    scores <- Y %*% cargas

.. code:: r

    pairs(scores,main="Scores",col="blue",pch=19)



.. image:: output_83_0.png
   :width: 420px
   :height: 420px

